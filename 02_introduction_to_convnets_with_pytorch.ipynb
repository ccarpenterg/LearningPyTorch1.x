{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_introduction_to_convnets_with_pytorch.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ccarpenterg/LearningPyTorch1.x/blob/master/02_introduction_to_convnets_with_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKWpH3rxHD5P",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Convolutional Neural Networks with PyTorch\n",
        "\n",
        "Similar to traditional Neural Networks, Convolutional Neural Networks are built using neurons but instead of only fully connected layers, convolutional networks also have convolutional layers.\n",
        "\n",
        "In general, convnets consist of two parts: a convolutional base and a fully connected classifier. The convolutional base automatically extract the features that are subsequently feed to a dense classifier, which outputs the probabilities of an image to belong to a certain class.\n",
        "\n",
        "\n",
        "So let's start by importing some standard modules and the MNIST dataset module. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqLIXx67Ol0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import statistics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd4dpSQiTjml",
        "colab_type": "text"
      },
      "source": [
        "PyTorch is installed in Colab by default, but it's always a good practice to check what version we'll be working with. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81sTn8Eoeo1s",
        "colab_type": "code",
        "outputId": "9e6b7af0-caa8-4f4f-a3b4-fe541acd1098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('PyTorch version:', torch.__version__)\n",
        "print('Torchvision version:', torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: 1.1.0\n",
            "Torchvision version: 0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1LQTo5gSLCa",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional and Pooling Layers\n",
        "\n",
        "A convolutional layer using pyTorch:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "torch.nn.Conv2d(num_in_channels, num_out_channels, kernel_size)\n",
        "```\n",
        "\n",
        "num_in_channels is the number of channels of the input tensor. If the previous layer is the input layer, num_in_channels is the number of channels of the image (3 channels for RGB images), otherwise num_in_channels is equal to the number of feature maps of the previous layer.\n",
        "\n",
        "num_out_channels is the number of filters (feature extractor) that this layer will apply over the image or feature maps generated by the previous layer.\n",
        "\n",
        "So for instance, if we have an RGB image and we are going to apply 32 filters of 3x3:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "torch.nn.Conv2d(3, 32, 3)\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bo9I8y4XRn2h",
        "colab_type": "text"
      },
      "source": [
        "## A Simple Convolutional Neural Network\n",
        "\n",
        "In our convnet we'll use the next structure:\n",
        "\n",
        "*input -> convolution -> pooling-> convolution -> pooling -> convolution* (convolutional base)\n",
        "\n",
        "*fully connected -> fully connected -> output*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6IZxtIn5CnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BasicCNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_channels, num_classes):\n",
        "        super(BasicCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(num_channels, 32, 3, stride=1, padding=0)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=0)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1, padding=0)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.fc1 = nn.Linear(3*3*64, 64, bias=True)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        x = F.relu(self.conv1(X))\n",
        "        x = self.pool1(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.reshape(-1, 3*3*64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1OqK7nx8gs",
        "colab_type": "text"
      },
      "source": [
        "**Convolution #1**\n",
        "\n",
        "32 kernels of 3x3\n",
        "\n",
        "(28 - 3 + 2x0) / 1 + 1 = 26\n",
        "\n",
        "Output dimensions: (32, 26, 26)\n",
        "\n",
        "**Max Pooling #1**\n",
        "\n",
        "filter size = 2, stride = 2\n",
        "\n",
        "(26 - 2) / 2 + 1 = 13\n",
        "\n",
        "Output dimensions: (32, 13, 13)\n",
        "\n",
        "**Convolution #2**\n",
        "\n",
        "64 kernels of 3x3\n",
        "\n",
        "(13 - 3 + 2x0) / 1 + 1 = 11\n",
        "\n",
        "Output dimensions: (64, 11, 11)\n",
        "\n",
        "**Max Pooling #2**\n",
        "\n",
        "filter size = 2, stride = 2\n",
        "\n",
        "(11 - 2) / 2 + 1 = 5\n",
        "\n",
        "Output dimensions: (64, 5, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9iaMJw9SlqO",
        "colab_type": "text"
      },
      "source": [
        "We'll send the model to our GPU so we need to create a CUDA device and instantiate our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ws1WdSBASyf9",
        "colab_type": "code",
        "outputId": "171d046c-4a18-4c33-a7b7-69027e88b705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "cuda = torch.device('cuda')\n",
        "\n",
        "model = BasicCNN(1, 10)\n",
        "model.to(cuda)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicCNN(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Linear(in_features=576, out_features=64, bias=True)\n",
              "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTiUSbCjgrbd",
        "colab_type": "text"
      },
      "source": [
        "## MNIST Datatset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uHVwNkggvi-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.1307], [0.3081])\n",
        "])\n",
        "\n",
        "train_set = MNIST('./mnist', train=True, download=True, transform=dataset_transform)\n",
        "valid_set  = MNIST('./mnist', train=False, download=True, transform=dataset_transform)\n",
        "\n",
        "\n",
        "#let's check the size of our tensors\n",
        "print(train_set.data.shape)\n",
        "print(valid_set.data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4KWaVnIRLBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(train_set, batch_size=128, num_workers=0, shuffle=True)\n",
        "valid_loader = DataLoader(valid_set, batch_size=512, num_workers=0, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpTDKCIHR_I8",
        "colab_type": "text"
      },
      "source": [
        "We now create a dummy matrix x to simulate the input of a MNIST image, and check we get the right output in terms of dimensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAYeFCSISUoS",
        "colab_type": "code",
        "outputId": "2d3985ae-f7e9-404c-ec63-ac34b496b941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://pytorch.org/docs/stable/nn.html#conv2d\n",
        "# input: (N, C_in, H, W) -> N: batch, C_in: number of channels, H: height, W: width\n",
        "x = torch.randn(128, 1, 28, 28, device=cuda)\n",
        "output = model(x)\n",
        "print(output.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([128, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efssm5NpdmjU",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUfaoQ8TrhKu",
        "colab_type": "text"
      },
      "source": [
        "**Optimizer: Stochastic Gradient Descent**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hycybwJOsrIO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
        "# Stochastic gradient descent optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J2UGrKFl34q",
        "colab_type": "text"
      },
      "source": [
        "**Train function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3qvlx6bdq30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, loss_fn, optimizer):\n",
        "    \n",
        "    #set the module in training mode\n",
        "    model.train()\n",
        "    \n",
        "    train_batch_losses = []\n",
        "    \n",
        "    for batch, labels in train_loader:\n",
        "        \n",
        "        #send the training data to the GPU\n",
        "        batch = batch.to(cuda)\n",
        "        labels = labels.to(cuda)\n",
        "        \n",
        "        #set all gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #forward propagate\n",
        "        y_pred = model(batch)\n",
        "        \n",
        "        #calculate the loss\n",
        "        loss = loss_fn(y_pred, labels)\n",
        "        \n",
        "        #bachpropagate\n",
        "        loss.backward()\n",
        "        \n",
        "        #update the parameters (weights and biases)\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_batch_losses.append(float(loss))\n",
        "        \n",
        "        mean_loss = statistics.mean(train_batch_losses)\n",
        "        \n",
        "    return mean_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kie8k4b7l8JJ",
        "colab_type": "text"
      },
      "source": [
        "**Validation function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPRKOrZUkMYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, loss_fn, optimizer):\n",
        "    \n",
        "    # set the model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # save predictions for later\n",
        "    pedrictions = []\n",
        "    \n",
        "    # stop tracking the parameters for backpropagation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        validation_batch_losses = []\n",
        "        \n",
        "        for batch, labels in valid_loader:\n",
        "            \n",
        "            # send the validation data to GPU\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            # forward propagate\n",
        "            labels_pred = model(batch)\n",
        "            \n",
        "            # calculate loss\n",
        "            loss = loss_fn(labels_pred, labels)\n",
        "            \n",
        "            validation_batch_losses.append(float(loss))\n",
        "            \n",
        "            mean_loss = statistics.mean(validation_batch_losses)\n",
        "           \n",
        "    return mean_loss "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE6HMNEmbWrh",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy function**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFiCRnYyWZ2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(model, loader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch, labels in loader:\n",
        "            batch = batch.to(cuda)\n",
        "            labels = labels.to(cuda)\n",
        "            \n",
        "            labels_pred = model(batch)\n",
        "            \n",
        "            _, predicted = torch.max(labels_pred.data, 1)\n",
        "        \n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            \n",
        "            return (100 * correct / total)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YZuWHMoUitI",
        "colab_type": "text"
      },
      "source": [
        "**Training our Convolutional Neural Network**\n",
        "\n",
        "Now it's time to train our brand new convolutional neural network. We'll use the cross entropy function as our loss function (in pytorch softmax is included in the cross entropy function)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MF10VSf-ZvcC",
        "colab_type": "code",
        "outputId": "b8d70d6f-1d23-445b-ba84-da0297d4be8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for epoch in range(1, 1+10):\n",
        "    \n",
        "    print('Epoch number', epoch)\n",
        "    \n",
        "    train_loss = train(model, loss_fn, optimizer)\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    print('Training loss:', train_loss)\n",
        "    print('Training accuracy: {}%'.format(accuracy(model, train_loader)))\n",
        "    \n",
        "    valid_loss = validate(model, loss_fn, optimizer)\n",
        "    valid_losses.append(valid_loss)\n",
        "    \n",
        "    print('Validation loss:', valid_loss)\n",
        "    print('Validation accuracy: {}%'.format(accuracy(model, valid_loader)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number 1\n",
            "Training loss: 0.020068374754332784\n",
            "Training accuracy: 99.21875%\n",
            "Validation loss: 0.046191389905288815\n",
            "Validation accuracy: 99.21875%\n",
            "Epoch number 2\n",
            "Training loss: 0.021534487716297606\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.06009956393390894\n",
            "Validation accuracy: 98.828125%\n",
            "Epoch number 3\n",
            "Training loss: 0.023590450347867855\n",
            "Training accuracy: 99.21875%\n",
            "Validation loss: 0.044990355311892924\n",
            "Validation accuracy: 99.609375%\n",
            "Epoch number 4\n",
            "Training loss: 0.017455021950735478\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.05655718655325472\n",
            "Validation accuracy: 98.4375%\n",
            "Epoch number 5\n",
            "Training loss: 0.020203706586479284\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.06522305025719106\n",
            "Validation accuracy: 99.4140625%\n",
            "Epoch number 6\n",
            "Training loss: 0.018918860306554257\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.061015042616054414\n",
            "Validation accuracy: 97.8515625%\n",
            "Epoch number 7\n",
            "Training loss: 0.021324404368876046\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.07924528261646628\n",
            "Validation accuracy: 98.828125%\n",
            "Epoch number 8\n",
            "Training loss: 0.017331979912656077\n",
            "Training accuracy: 98.4375%\n",
            "Validation loss: 0.05236246306449175\n",
            "Validation accuracy: 99.0234375%\n",
            "Epoch number 9\n",
            "Training loss: 0.018805425978505978\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.08254654724150896\n",
            "Validation accuracy: 99.21875%\n",
            "Epoch number 10\n",
            "Training loss: 0.02364912809285798\n",
            "Training accuracy: 100.0%\n",
            "Validation loss: 0.053402185323648155\n",
            "Validation accuracy: 99.0234375%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}